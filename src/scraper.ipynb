{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f109b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import csv\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363da9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://horizon.fandom.com\"\n",
    "TARGET_URL = f\"{BASE_URL}/wiki/Special:AllPages\"\n",
    "HEADERS = {'User-Agent': 'Mozilla/5.0'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "246f6011",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content(soup):\n",
    "    infobox = '[ Infobox source ]'\n",
    "    content_parts = []\n",
    "    content_div = soup.find(\"div\", class_=\"mw-parser-output\")\n",
    "    if content_div:\n",
    "        for p in content_div.find_all(\"p\", recursive=False):\n",
    "            text = p.get_text(\" \", strip=True)\n",
    "            if text:\n",
    "                content_parts.append(text)\n",
    "    content = \"\\n\".join(content_parts)\n",
    "    idx = content.find(infobox)\n",
    "    return content[idx+len(infobox):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5248d284",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_location(soup):\n",
    "    location = None\n",
    "    loc_div = soup.find(attrs={\"data-source\": \"location\"})\n",
    "    if loc_div:\n",
    "        a = loc_div.find(\"a\")\n",
    "        location = a.get_text(strip=True) if a else loc_div.get_text(strip=True)\n",
    "    return location\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7764c8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_category(soup):\n",
    "    cat_div = soup.find(attrs={\"data-source\": \"category\"})\n",
    "    if cat_div:\n",
    "        val = cat_div.find(class_=\"pi-data-value\") or cat_div\n",
    "        a = val.find(\"a\")\n",
    "        if a and a.get_text(strip=True):\n",
    "            return a.get_text(strip=True)\n",
    "        return val.get_text(\" \", strip=True).replace(\"Category\", \"\", 1).strip()\n",
    "    foot = soup.select_one(\"#mw-normal-catlinks ul li a\")\n",
    "    return foot.get_text(strip=True) if foot else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d41d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 345/345 [01:52<00:00,  3.05it/s]\n",
      "100%|██████████| 345/345 [01:44<00:00,  3.31it/s]\n",
      "100%|██████████| 345/345 [02:20<00:00,  2.45it/s]\n",
      "100%|██████████| 345/345 [02:04<00:00,  2.77it/s]\n",
      "100%|██████████| 345/345 [01:59<00:00,  2.89it/s]\n",
      "100%|██████████| 345/345 [02:20<00:00,  2.46it/s]\n",
      "100%|██████████| 345/345 [02:22<00:00,  2.42it/s]\n",
      "100%|██████████| 345/345 [02:16<00:00,  2.53it/s]\n",
      "100%|██████████| 345/345 [02:20<00:00,  2.46it/s]\n",
      "100%|██████████| 345/345 [02:14<00:00,  2.56it/s]\n"
     ]
    }
   ],
   "source": [
    "def scrape_data():\n",
    "    # Get the current datetime object\n",
    "    current_datetime = datetime.now()\n",
    "\n",
    "    # Format the datetime object into a string\n",
    "    date_time_string = current_datetime.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    main_page = requests.get(f\"{TARGET_URL}\", headers=HEADERS)\n",
    "    main_soup = BeautifulSoup(main_page.text, 'html.parser')\n",
    "    next_page = main_soup.find(\"a\", string=lambda text: text and text.startswith(\"Next page\"))\n",
    "\n",
    "    with open(f\"horizon_data_{date_time_string}.csv\", 'a', newline='') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow(['address', 'category', 'location', 'content'])\n",
    "        while not next_page is None :\n",
    "            page_links = main_soup.find_all('div', attrs=\"mw-allpages-body\")[0].find_all('a')\n",
    "            for link in tqdm(page_links):\n",
    "                link = link.get('href')\n",
    "                new_page = requests.get(f\"{BASE_URL}/{link}\", headers=HEADERS)\n",
    "                new_soup = BeautifulSoup(new_page.text, 'html.parser')\n",
    "                writer.writerow([link, get_category(new_soup), get_location(new_soup), get_content(new_soup)])\n",
    "        \n",
    "            main_page = requests.get(f\"{BASE_URL}/{next_page.get('href')}\", headers=HEADERS)\n",
    "            main_soup = BeautifulSoup(main_page.text, 'html.parser')\n",
    "            next_page = main_soup.find(\"a\", string=lambda text: text and text.startswith(\"Next page\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "horizonzerodawn-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
